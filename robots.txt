# File: robots.txt
# Author: Zachary Cutlip, uid000-at-gmail-dot-com
# License: MIT
# Purpose: A 'robots.txt' file to block generative AI scrapers from stealing
#          your website's content. Feel free to copy and use whichever entries
#          you like, ignoring whichever entries you care to. The freedom to
#          block or share your own content is in your hands.
#
# Note: Nothing here actually prevents scrapers from stealing your content.
#       'robots.txt' is a gentleman's agreement between websites and crawlers.
#       There's nothing technical to force crawlers to respect this file.

# OpenAI GPTBot crawler (https://platform.openai.com/docs/gptbot)
User-agent: GPTbot
Disallow: /

# OpenAI ChatGPT service (https://platform.openai.com/docs/plugins/bot)
User-agent: ChatGPT-User
Disallow: /

# Google Bard / Gemini crawler (https://developers.google.com/search/docs/crawling-indexing/overview-google-crawlers)
User-agent: Google-Extended
Disallow: /

## AppleBot crawler
User-agent: Applebot-Extended
Disallow: /

# Perplexity AI
# https://archive.is/22gCl (wired.com)
# https://rknight.me/blog/perplexity-ai-is-lying-about-its-user-agent/
User-Agent: PerplexityBot
Disallow: /

# Anthropic AI
# https://support.anthropic.com/en/articles/8896518-does-anthropic-crawl-data-from-the-web-and-how-can-site-owners-block-the-crawler
User-agent: ClaudeBot
Disallow: /

# Amazonbot (Alexa)
# https://developer.amazon.com/en/amazonbot
User-agent: Amazonbot
Disallow: /

# Bytespider
# https://darkvisitors.com/agents/bytespider
User-agent: Bytespider
Disallow: /

# Common Crawl
# https://commoncrawl.org/ccbot
User-agent: CCBot
Disallow: /

# Diffbot
# https://docs.diffbot.com/reference/crawl-introduction
User-agent: Diffbot
Disallow: /

# Facebook (FacebookBot)
# https://developers.facebook.com/docs/sharing/bot/
User-agent: FacebookBot
Disallow: /
